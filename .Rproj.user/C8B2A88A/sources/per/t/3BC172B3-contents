---
title: "Deep Learning Get Started"
author: "  "
date: ''
output:
  html_document:
    code_folding: show
    highlight: haddock
    number_sections: yes
    theme: lumen
    toc: yes
    toc_depth: 4
    toc_float: yes
  pdf_document:
    number_sections: yes
    toc: yes
    toc_depth: '4'
urlcolor: blue
editor_options: 
  chunk_output_type: inline
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, cache = T)
knitr::opts_chunk$set(fig.height=4, fig.width=7, fig.align = 'center', warning = F)

if(!require('pacman')) {
  install.packages('pacman')
}
pacman::p_load(keras, ggplot2, glmnet, RColorBrewer, wordcloud, neuralnet, latex2exp, data.table)
```


\pagebreak

# Introduction {-}

Table of Contents

1. Installation
    + Python
    + Tensorflow and Keras
2. First neural network
    + Case study: Yelp reviews
    + Build a neural network
    + Train the neural network
    + Evaluate accuracy of the model
    + Save and load the model
        

The goal of this file is to guide you install the essentials for deep learning and get you started. We will first install **Python** (another programming language) and then two deep learning toolboxes, **Tensorflow** and **Keras**. 

After all the installation, try to run this file. 


# Python, Tensorflow, and Keras

## Install Python

Here are the installation guides for different platforms. [Download conda for your platform](https://www.anaconda.com/products/individual) (it is in the Anaconda Installers section) and follow the step in the installer.

* MacOS: https://conda.io/projects/conda/en/latest/user-guide/install/macos.html
* Windows: https://conda.io/projects/conda/en/latest/user-guide/install/windows.html
* Linux: https://conda.io/projects/conda/en/latest/user-guide/install/linux.html

*Python* is another commonly-used programming language, especially in machine learning community. A lot of popular libraries, including Tensorflow and Keras that we will use in this lecture, are developed in Python. Hence, we need to install Python first and use R interfaces to interact with those libraries. Here's the [link](https://www.anaconda.com/distribution/) to Python Anaconda distribution (version). Anaconda is a package manager and provides a one stop shop to install Python (and R) and some of the most popular libraries for data science. Anaconda Python also optimizes for numerical computation (because it is built against Intel's math kernel library). Note that for Windows users, in order to use Kera, you have to install Anaconda distribution; otherwise for Mac/Linux users, you can either use Anaconda Python or [Vanilla Python](https://www.python.org/downloads/).

## Tensorflow and Keras

After we install Python, we can come back to R and let R install Tensorflow and Keras.

### Tensorflow

First, install the tensorflow R package. 
```{r}
if(!require("tensorflow")) remotes::install_github("rstudio/tensorflow")
```

Then, use the `install_tensorflow()` function to install TensorFlow. 

```{r, eval = F}
library(tensorflow)
install_tensorflow()
```

For the new Mac with Apple silicon, the installation may not be as straightforward (even though the development version of `tensorflow` package supports Apple silicon). Please follow the instruction if `install_tensorflow()` does not work:

i. Install **R for Apple silicon arm64** from [here](https://cran.r-project.org/bin/macosx/) 
ii. Follow Step 1 to 4  [here](https://caffeinedev.medium.com/how-to-install-tensorflow-on-m1-mac-8e9b91d93706) to install Tensorflow 
iii. Run the following chunk to setup **every time** before running tensorflow

```{r, eval=F}
library(tensorflow)
# run this line if you are using Apple silicon arm64
use_condaenv('mlp') # change 'mlp' according to the name of your virtual environment in Step ii.3.

# Sys.setenv(RETICULATE_PYTHON = "~/miniforge3/envs/r/bin/python")
```

iv. (Optional) You can make the above chunk as the default setup as follows so that you don't need to run it every time.

    a. In terminal, `open ~/.Rprofile`. If there is no .Rprofile, create one first by `touch ~/.Rprofile`. (R will run the code in ~/.Rprofile every time you start).
    b. Append the above code into the file and save.
    c. Restart R.
    
    
You can confirm that the installation succeeded with:
```{r}
library(tensorflow)
tf$constant("Hellow Tensorflow")
```

### Keras

Similarly for Keras, install the Keras R package.
```{r}
if(!require("keras")) install.packages("keras")
```

Then, use the `install_keras()` function to install Keras
```{r, eval = F}
library(keras)
install_keras()
```

*TensorFlow* is a machine learning library developed by Google. *Keras* is a high-level Application Programming Interface (API) for TensorFlow to build and train deep learning models. It runs on top of TensorFlow and is used for fast prototyping, advanced research, and production. An R interface to Keras by RStudio is available. Here is an official [installation guide](https://tensorflow.rstudio.com/installation/). 

For more information about TensorFlow in R: https://tensorflow.rstudio.com/
    

# Case study: Yelp Textming via neural network

How well can we predict ratings from a review? The Yelp data available to us here is 100000 previous reviews together with ratings. Use bag of words, each review is already processed in the term frequency format. There are  $p=1072$ words retained. The response variable is a binary `rating`, 1 being a good review and 0 being a bad review. 

In this section we will implement a Neural Network model with two layers and varying number of neutrons. Package `keras` is used. 


## Data Preparation

**Read the data**:  `YELP_tm_freq.csv` is a processed Yelp data. It contains the response `ratings`and reviews. `data3` extracted: the first variable is the response `rating` and the remaining variables are word frequencies or input of $x_1, \ldots, x_{1072}$.    

```{r results=FALSE}
# Data prep
data2 <- fread("data/YELP_TM_freq.csv")  # fread the term freq tables
names(data2)[c(1:5, 500:510)] # notice that user_id, stars and date are in the data2
dim(data2)
data3 <- data2[, -c(1:3)]; dim(data3)# the first element is the rating
names(data3)[1:3]
levels(as.factor(data3$rating))

```

**Data preparation for NN**:

  + We need to split data into two sets: training data and validation
  + Training data will be split internally to tune any parameters
  + We reserve the validation data set to give an honest evaluation for the testing error. That is the only time it will be used.
  + All the data **need** to be either matrix or vectors.


**Validation data: `data3_val`**: reserve 10,000
```{r results='hide'}
# Split data
set.seed(1)  # for the purpose of reproducibility
n <- nrow(data3)
validation.index <- sample(n, 10000)
length(validation.index)   # reserve 10000
data3_val <- data3[validation.index, ] # 
## validation input/y
data3_xval <- as.matrix(data3_val[, -1])  # make sure it it is a matrix
data3_yval <- as.matrix(data3_val[, 1]) # make sure it it is a matrix
```

**Training data: `data3_xtrain`/`data3_ytrain`**: Use 90,000 as the training data. Internally we need to split this training data again to tune the parameters. Keras refer the split internal datasets as training/validation. (We have call this pair training/testing)

```{r}
## training input/y: need to be matrix/vector
data3_xtrain <- data3[-validation.index, -1]   #dim(data3_xtrain)
data3_ytrain <- data3[-validation.index, 1]   
data3_xtrain <- as.matrix(data3_xtrain) # make sure it it is a matrix
data3_ytrain <- as.matrix(data3_ytrain) # make sure it it is a matrix
```



## Fully Connected Neural Network

Now the data is prepared, let's begin working with our full dataset using the `keras` package. The first step is to define our model. By defining, we mean specifying the number and type of layers to include in our model and what will happen to our data in each layer. For our purposes `keras_model_sequential()` is the function to run when defining our model. 

To add a fully connected or dense layer we use the command `layer_dense()`.

When building the layer, the shape of the input needs to be specified. This refers to the length of each input vector, which in our case is 1072 (the 1072 possible words in our frequency dictionary).

We also need to specify how many hidden units our layer will contain (these are represented by the nodes or neurons in the earlier graphic). Having more hidden units (a higher-dimensional representation space) allows your network to learn more-complex representations, but it makes the network more computationally expensive and may lead to learning unwanted patterns. Here we use 16 neurons in the first layer and 8 neurons in the second layer. 

Finally, we need to specify the activation function. Here we use the *rectified linear unit* or ReLU function mentioned earlier. This has the effect of zeroing out negative values (equivalent to $\max(0, X)$).

The final layer of the model specifies our output. As this is a classification problem we want to find $P(Y=1)$. To set our output as a probability we specify the activation to be the "sigmoid" function (which as you recall is just the logistic function for which we are familiar.)

**Define the Model/Architecture:**
  + two layers with 16 and 8 neurons in each layer
  + Activation function is `Relu`
  + Output layer is `Sigmoid`
  
```{r}
# set seed for keras
set_random_seed(10)

p <- dim(data3_xtrain)[2] # number of input variables
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  
  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output
print(model)
```

As we can see in the table above our model has a total of **17322** parameters:    

  + The input to the model are yelp reviews that are each coded as 1072 length sequences of frequencies. 
  + Our model's first layer is 16 nodes that are fully connected    
  + At each node a different set of weights $W's$ will be applied to each value of the 1072 word sequence (i.e. each node will have 1072 weights and the network will have (1072 +1) * 16 = 17168 weights total) to compute the weighted sum to which a bias will be added and then the activation function will be applied.  
  + These values will then flow to our second layer with (16+1)*8=136 where weights will be applied to each value (16 weights total) and the weighted sum computed, the bias value will then be added and the activation function will then be applied 
  + The final layer which is output will have (8+1)*2=18 parameters
  + Combined our model with two layers and one final output, there are a total of 17168+136+18=17322 parameters across the model or architecture.   

Next we need to compile our model. This step refers to specifying the optimizer we want to use (we will always use 'rmsprop'), the loss function (since this is a binary classification problem we use 'binary_crossentropy')  and the metric used to evaluate the performance. (here we use accuracy which is the fraction of reviews that are correctly classified).

**Compile the Model**

```{r}
##Compile the Model
model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)
```

Before we fit our model, let us explain internally how the model is trained. The data will be split to

- training data
- validation data (we call this testing data in our class)

For example we may want to use 85% of the data as an internal training data and the remaining data as a validation data by specifying `validation_split = 0.15`. The training data will be used to get all the parameters and the loss and accuracy will be evaluated by the internal validation data.


We can now fit our model. The batch size refers to the number of samples per batch. This is part of the trick used to update the gradients with a batch number of reviews. The epoch refers to the number of iterations over the training data. Each epoch we cycle through all the data points one batch at a time. For test purposes we use 20 epochs. 

**Note that compile() and fit() modify the model object in place, unlike most R functions.** So after we compiled and fitted `model`, `model` has already changed. If we fit again on a fitted model, it will not retrain the model again. The output of the `fit()` function is history accuracy and loss of training process. So the `fit1` in the following chunk will store the training history but NOT the `model` object.

```{r}
fit1 <- model %>% fit(
  data3_xtrain,
  data3_ytrain,
  epochs = 20,
  batch_size = 512,
  validation_split = .15 # set 15% of the data3_xtain, data3_ytrain as the validation data
)

plot(fit1)
```


**Predictions:**
Lets see if we can gain further intuition by manually using the above parameter values for prediction. Below are the predicted probabilities from the model for the first five values in our training set.
```{r}
round(model %>% predict(data3_xtrain[1:5,]), 3)
```



**All 17323 parameters**
With our model now fit, lets take a look at the values for the 17313 parameters in our model.
```{r}
weights <- model %>% get_weights()
# str(weights) # show each layers of W's and b's
# hist(weights[[1]])   # W^(1)
# weights[[2]] # b's for layer 1
```


**fit1$metrics keeps all the evaluations**

```{r}
fit1$metrics$loss[20]  # fit1$metrics keeps all the evaluations
```


### Tuning Parameter Selection  

From the graph below we see that by about 6 epochs our validation loss has bottomed out and we receive no further benefit from additional iterations

```{r}
plot(fit1)
```

To avoid overfitting lets use 6 epochs in our final model. (We could also play with unit numbers and batch size to see if this has an impact). Now that we are ready to fit our final model we can use all of training data.

**Final training with all the training data:**

Here we have put all the steps together to get the final NN predictive equation

* Training data: data3_xtrain, data3_ytrain
* Validation data: data3_xvalidation, data3_yvalidation
* NN model:
  + two layers with 16 and 8 neurons in each layer
  + Activation function is `Relu`
  + Output layer is `Sigmoid`
* Epoch is 6


```{r}
p <- dim(data3_xtrain)[2] # number of input variables

#retain the nn:
model <- keras_model_sequential() %>%
  layer_dense(units = 16, activation = "relu", input_shape = c(p)) %>% 
  # 1 layer with 16 neurons. default activation is relu
  layer_dense(units = 8, activation = "relu") %>%  # layer 2 with 8 neurons
  layer_dense(units = 2, activation = "softmax") # output

model %>% compile(
  optimizer = "rmsprop",
  loss = "sparse_categorical_crossentropy",
  metrics = c("accuracy")
)

 model %>% fit(data3_xtrain, data3_ytrain, epochs = 6, batch_size = 512)
```


### Save the NN's

Now we can save our Keras model for later use.
```{r}
save_model_tf(object = model, filepath = "Yelp_model")
```

Then we can load the model. 

```{r}
Yelp_model <- load_model_tf(filepath = "Yelp_model")
```



### Assessing Performance  

We can use the evaluate function to assess our performance on the validation data to report a final performance of the model built. 
```{r warning=F, message=F}
results <- Yelp_model %>% evaluate(data3_xval, data3_yval)
results
```

Our accuracy on the validation data is an impressive 81% (19% of mis-classification error) with only two fully connected layers! Meaning that we correctly classified 80% of the reviews as positive or negative (this is a misclassification error of 20). With additional layers and different types of layers we could improve on this even further.

### Prediction  

Finally we can do prediction. Let us see how well we predict the first 5 reviews in the validation data. 

**Get probabilities:**
```{r}
pred.prob <- Yelp_model %>% predict(data3_xval[1:5,])
pred.prob
```

**Get the labels:**
```{r}
y.pred <- Yelp_model %>% predict(data3_xval[1:5,]) %>% k_argmax() %>% as.integer() # majority vote!
data.frame(yhat=y.pred, y=data3_yval[1:5, 1])
```


# Summary and Remarks {-}

We have introduced neural network and implemented it using keras. Neural network provides a non-linear model to predict response variables. They can be categorical or continuous responses. 

We have run Yelp review case study with dense layers. Since we have used bag of words to process the data. So the advantage of Neural nets are not seen here. 

Many open questions re NN models

* **How many layers and how deep within each layer?**
* **Which activation function to use in each layer?**

Often we may adopt existing successful studies

* **Apply their architecture**
* Often we may input their weights as initial values 


